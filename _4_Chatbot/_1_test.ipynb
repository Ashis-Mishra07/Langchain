{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot\n",
    "\n",
    "- We will build a LLm-powered Chatbot . This will be able to have a conversation and remember previous interaction .\n",
    "- The chatbot is built on only the use of language model to have a conversation . There are several other related concepts too .\n",
    "\n",
    "The idea involve \n",
    "- Conversational RAG : Enable a chatbot experience over an extrenal source of data .\n",
    "- Agent : Build a chatbot that can take actions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020FE4F2D9C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020FE4F2E6E0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = \"Gemma2-9b-It\" , groq_api_key=groq_api_key)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ashis,\\n\\nIt's great to meet you!  That's awesome that you're an upcoming software engineer.  \\n\\nWhat kind of software engineering are you most interested in?  Do you have a favorite language or area you're focusing on?\\n\\nI'm happy to chat about anything related to software engineering, or just general tech topics if you'd like.  \\n\\nLooking forward to hearing more about you! üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 24, 'total_tokens': 120, 'completion_time': 0.174545455, 'prompt_time': 0.002127426, 'queue_time': 0.23692686500000001, 'total_time': 0.176672881}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-567f0fb9-f290-46d1-b049-7287fda0235c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 96, 'total_tokens': 120})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're Ashis, and you're an upcoming software engineer!  \\n\\nIt's nice to know a little bit about you.  \\n\\nWhat kind of software engineering are you most interested in? üíª  What are you working on right now?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 74, 'total_tokens': 130, 'completion_time': 0.101818182, 'prompt_time': 0.007177035, 'queue_time': 0.234395185, 'total_time': 0.108995217}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-99035500-0b22-4feb-abc9-a86a17fe6906-0', usage_metadata={'input_tokens': 74, 'output_tokens': 56, 'total_tokens': 130})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "Here AI is remembering the prev context that u have given and it is responding the human asked question accordingly .\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \") ,\n",
    "    AIMessage(content = \"Hi Ashis,\\n\\nIt's great to meet you!  That's awesome that you're an upcoming software engineer.\") ,\n",
    "    HumanMessage(content = \"Hey say what's my name and what do I do ?\") \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History \n",
    "- This will actually store the prev msgs in some database , and if any user ask for any question . So based ont the prev response it has stored , it will give out the result .\n",
    "- This actually acts as a wrapper which make it stateful and track the inputs and outputs of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.45)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.15)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "ChatMessageHistory ‚Üí Stores chat history for a particular session.\n",
    "BaseChatMessageHistory ‚Üí A base class for handling chat histories.\n",
    "RunnableWithMessageHistory ‚Üí Allows chaining chat history with a LangChain model.\n",
    "'''\n",
    "\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}   # to store the session id \n",
    "\n",
    "''' \n",
    "Created session id's to distinguish between the chat history of different users.\n",
    "'''\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)   # chain between the modela and get_session_history\n",
    "config = {\"configurable\" : {\"session_id\" : \"chat1\"}}   #hardcoded value \n",
    "\n",
    "response = with_message_history.invoke([\n",
    "    HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")\n",
    "] , config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ashis, it's great to meet you!  That's awesome that you're pursuing a career in software engineering. It's a really exciting field with lots of possibilities.\\n\\nWhat areas of software engineering are you most interested in? Web development, mobile apps, game development, data science?  \\n\\nI'm happy to chat more about it and see if I can be of any help as you start your journey.  Good luck!\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're Ashis, and you're an upcoming software engineer!  üòä  \\n\\nIs there anything else you'd like to tell me about yourself or your software engineering goals? I'm happy to listen and learn more.  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 273, 'total_tokens': 326, 'completion_time': 0.096363636, 'prompt_time': 0.010621049, 'queue_time': 0.23753139, 'total_time': 0.106984685}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-46c012cd-b8bf-4c88-81b2-e7f048d04a35-0', usage_metadata={'input_tokens': 273, 'output_tokens': 53, 'total_tokens': 326})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now other question if I ask \n",
    "\n",
    "with_message_history.invoke(\n",
    "    [\n",
    "    HumanMessage(content = \"Hey say what's my name and what do I do ?\")\n",
    "    ] , config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now changing the session id \n",
    "config1 = {\"configurable\" : {\"session_id\": \"chat2\"}}\n",
    "resp = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content = \"Hey say what's my name and what do I do ?\")\n",
    "    ]\n",
    "    ,config=config1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name or what you do.\\n\\nIf you'd like to tell me, I'm happy to learn! üòÑ  \\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.content\n",
    "\n",
    "# As an AI, I have no memory of past conversations and do not know your name or what you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This all mean that the AI is storing the corresponding session id and the question will be answered as per the stored content only . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to remove the functionality of manual giving in the form of list , and pass by giving Placeholder with key as messages\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder \n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\" , \"You are a helpful assistant . Answer all the question to the best of your ability\") , \n",
    "#         MessagesPlaceholder(variable_name=\"messages\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# To add more complexcity to can add like \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\" , \"You are a helpful assistant . Answer all the question to the best of your ability in {language}\") , \n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ashis, it's nice to meet you!  It's great to hear you're an upcoming software engineer. That's an exciting field to be in. \\n\\nI'm here to help in any way I can.  What can I do for you today?\\n\\nDo you have any questions about software engineering, programming, or anything else? I can try my best to answer them!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 40, 'total_tokens': 128, 'completion_time': 0.16, 'prompt_time': 0.002334245, 'queue_time': 0.23235461, 'total_time': 0.162334245}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2a4c4bc-71a9-42d6-a56a-51c41b8d4e5e-0', usage_metadata={'input_tokens': 40, 'output_tokens': 88, 'total_tokens': 128})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.invoke({\"messages\":[HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")]})\n",
    "\n",
    "chain.invoke( \n",
    "    {\n",
    "        \"messages\":[HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")] ,\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history , input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\": \"chat4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content = \"Hey say what's my name and what do I do ?\")\n",
    "    ]\n",
    "    ,config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and don't know your name or what you do. \\n\\nIf you'd like to tell me, I'm happy to learn! üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 38, 'total_tokens': 85, 'completion_time': 0.085454545, 'prompt_time': 0.002506165, 'queue_time': 0.23866799, 'total_time': 0.08796071}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1fcf57a1-42b4-4b8e-a372-c7eac8e933ff-0', usage_metadata={'input_tokens': 38, 'output_tokens': 47, 'total_tokens': 85})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTrim-messages basically helps to reducing the number of messages we are sending to the model . The trimmer allows us to specify how many \\ntokens wr want to keep , along with other parameter like if we want to always keep the system message and whether allow partial messages .\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage , trim_messages\n",
    "''' \n",
    "Trim-messages basically helps to reducing the number of messages we are sending to the model . The trimmer allows us to specify how many \n",
    "tokens wr want to keep , along with other parameter like if we want to always keep the system message and whether allow partial messages .\n",
    "\n",
    "\n",
    "The function trim_messages() is used to limit the number of tokens in a chat history while keeping the conversation meaningful. \n",
    "It ensures that the model does not exceed token limits, which is crucial when dealing with LLMs that have token constraints.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=40,         # Limit the total message history to 70 tokens\n",
    "    strategy=\"last\",       # Keep the most recent messages; discard older ones\n",
    "    token_counter=model,   # Uses the model‚Äôs token counter to estimate tokens\n",
    "    include_system=True,   # Keep the system message (assistant instructions)\n",
    "    allow_partial=False,  # Do not allow cutting messages in half\n",
    "    start_on=\"human\"       # Start trimming from the human/user messages\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of messages \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"you're a good assistant\") ,\n",
    "    HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \") ,\n",
    "    AIMessage(content = \"Hi Ashis\") ,\n",
    "    HumanMessage(content = \"I like chocolate icecreme \") ,\n",
    "    AIMessage(content=\"nice\") ,\n",
    "    HumanMessage(content=\"I like to play cricket\") ,\n",
    "    AIMessage(content=\"That's awesome\") ,\n",
    "    HumanMessage(content=\"I am from India\") ,\n",
    "    AIMessage(content=\"That's cool\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like to play cricket', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's awesome\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am from India', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's cool\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you love to play **cricket**! üèè  \\n\\nIs there anything else you enjoy doing besides cricket?\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if using chain\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign( messages=itemgetter(\"messages\") | trimmer ) | prompt | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What I love to play ?\")],\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you are from India!  üáÆüá≥ üèè \\n\\n\\nLet me know if you have any other questions about cricket or anything else. üòä\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets wrap this message history \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\" : {\"session_id\": \"chat5\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What country I belongs to ?\")],\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    "    ,config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
