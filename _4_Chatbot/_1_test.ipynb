{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot\n",
    "\n",
    "- We will build a LLm-powered Chatbot . This will be able to have a conversation and remember previous interaction .\n",
    "- The chatbot is built on only the use of language model to have a conversation . There are several other related concepts too .\n",
    "\n",
    "The idea involve \n",
    "- Conversational RAG : Enable a chatbot experience over an extrenal source of data .\n",
    "- Agent : Build a chatbot that can take actions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020FE4F2D9C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020FE4F2E6E0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = \"Gemma2-9b-It\" , groq_api_key=groq_api_key)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ashis,\\n\\nIt's great to meet you!  That's awesome that you're an upcoming software engineer.  \\n\\nWhat kind of software engineering are you most interested in?  Do you have a favorite language or area you're focusing on?\\n\\nI'm happy to chat about anything related to software engineering, or just general tech topics if you'd like.  \\n\\nLooking forward to hearing more about you! üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 24, 'total_tokens': 120, 'completion_time': 0.174545455, 'prompt_time': 0.002127426, 'queue_time': 0.23692686500000001, 'total_time': 0.176672881}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-567f0fb9-f290-46d1-b049-7287fda0235c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 96, 'total_tokens': 120})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're Ashis, and you're an upcoming software engineer!  \\n\\nIt's nice to know a little bit about you.  \\n\\nWhat kind of software engineering are you most interested in? üíª  What are you working on right now?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 74, 'total_tokens': 130, 'completion_time': 0.101818182, 'prompt_time': 0.007177035, 'queue_time': 0.234395185, 'total_time': 0.108995217}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-99035500-0b22-4feb-abc9-a86a17fe6906-0', usage_metadata={'input_tokens': 74, 'output_tokens': 56, 'total_tokens': 130})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "Here AI is remembering the prev context that u have given and it is responding the human asked question accordingly .\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \") ,\n",
    "    AIMessage(content = \"Hi Ashis,\\n\\nIt's great to meet you!  That's awesome that you're an upcoming software engineer.\") ,\n",
    "    HumanMessage(content = \"Hey say what's my name and what do I do ?\") \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History \n",
    "- This will actually store the prev msgs in some database , and if any user ask for any question . So based ont the prev response it has stored , it will give out the result .\n",
    "- This actually acts as a wrapper which make it stateful and track the inputs and outputs of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.45)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.15)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "ChatMessageHistory ‚Üí Stores chat history for a particular session.\n",
    "BaseChatMessageHistory ‚Üí A base class for handling chat histories.\n",
    "RunnableWithMessageHistory ‚Üí Allows chaining chat history with a LangChain model.\n",
    "'''\n",
    "\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}   # to store the session id \n",
    "\n",
    "''' \n",
    "Created session id's to distinguish between the chat history of different users.\n",
    "'''\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)   # chain between the modela and get_session_history\n",
    "config = {\"configurable\" : {\"session_id\" : \"chat1\"}}   #hardcoded value \n",
    "\n",
    "response = with_message_history.invoke([\n",
    "    HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")\n",
    "] , config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ashis, it's great to meet you!  That's awesome that you're pursuing a career in software engineering. It's a really exciting field with lots of possibilities.\\n\\nWhat areas of software engineering are you most interested in? Web development, mobile apps, game development, data science?  \\n\\nI'm happy to chat more about it and see if I can be of any help as you start your journey.  Good luck!\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're Ashis, and you're an upcoming software engineer!  üòä  \\n\\nIs there anything else you'd like to tell me about yourself or your software engineering goals? I'm happy to listen and learn more.  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 273, 'total_tokens': 326, 'completion_time': 0.096363636, 'prompt_time': 0.010621049, 'queue_time': 0.23753139, 'total_time': 0.106984685}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-46c012cd-b8bf-4c88-81b2-e7f048d04a35-0', usage_metadata={'input_tokens': 273, 'output_tokens': 53, 'total_tokens': 326})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now other question if I ask \n",
    "\n",
    "with_message_history.invoke(\n",
    "    [\n",
    "    HumanMessage(content = \"Hey say what's my name and what do I do ?\")\n",
    "    ] , config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now changing the session id \n",
    "config1 = {\"configurable\" : {\"session_id\": \"chat2\"}}\n",
    "resp = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content = \"Hey say what's my name and what do I do ?\")\n",
    "    ]\n",
    "    ,config=config1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name or what you do.\\n\\nIf you'd like to tell me, I'm happy to learn! üòÑ  \\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.content\n",
    "\n",
    "# As an AI, I have no memory of past conversations and do not know your name or what you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This all mean that the AI is storing the corresponding session id and the question will be answered as per the stored content only . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to remove the functionality of manual giving in the form of list , and pass by giving Placeholder with key as messages\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder \n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\" , \"You are a helpful assistant . Answer all the question to the best of your ability\") , \n",
    "#         MessagesPlaceholder(variable_name=\"messages\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# To add more complexcity to can add like \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\" , \"You are a helpful assistant . Answer all the question to the best of your ability in {language}\") , \n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ashis, it's nice to meet you!  It's great to hear you're an upcoming software engineer. That's an exciting field to be in. \\n\\nI'm here to help in any way I can.  What can I do for you today?\\n\\nDo you have any questions about software engineering, programming, or anything else? I can try my best to answer them!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 40, 'total_tokens': 128, 'completion_time': 0.16, 'prompt_time': 0.002334245, 'queue_time': 0.23235461, 'total_time': 0.162334245}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2a4c4bc-71a9-42d6-a56a-51c41b8d4e5e-0', usage_metadata={'input_tokens': 40, 'output_tokens': 88, 'total_tokens': 128})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.invoke({\"messages\":[HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")]})\n",
    "\n",
    "chain.invoke( \n",
    "    {\n",
    "        \"messages\":[HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \")] ,\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history , input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\": \"chat4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content = \"Hey say what's my name and what do I do ?\")\n",
    "    ]\n",
    "    ,config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and don't know your name or what you do. \\n\\nIf you'd like to tell me, I'm happy to learn! üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 38, 'total_tokens': 85, 'completion_time': 0.085454545, 'prompt_time': 0.002506165, 'queue_time': 0.23866799, 'total_time': 0.08796071}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1fcf57a1-42b4-4b8e-a372-c7eac8e933ff-0', usage_metadata={'input_tokens': 38, 'output_tokens': 47, 'total_tokens': 85})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTrim-messages basically helps to reducing the number of messages we are sending to the model . The trimmer allows us to specify how many \\ntokens wr want to keep , along with other parameter like if we want to always keep the system message and whether allow partial messages .\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage , trim_messages\n",
    "''' \n",
    "Trim-messages basically helps to reducing the number of messages we are sending to the model . The trimmer allows us to specify how many \n",
    "tokens wr want to keep , along with other parameter like if we want to always keep the system message and whether allow partial messages .\n",
    "\n",
    "\n",
    "The function trim_messages() is used to limit the number of tokens in a chat history while keeping the conversation meaningful. \n",
    "It ensures that the model does not exceed token limits, which is crucial when dealing with LLMs that have token constraints.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=40,         # Limit the total message history to 70 tokens\n",
    "    strategy=\"last\",       # Keep the most recent messages; discard older ones\n",
    "    token_counter=model,   # Uses the model‚Äôs token counter to estimate tokens\n",
    "    include_system=True,   # Keep the system message (assistant instructions)\n",
    "    allow_partial=False,  # Do not allow cutting messages in half\n",
    "    start_on=\"human\"       # Start trimming from the human/user messages\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of messages \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"you're a good assistant\") ,\n",
    "    HumanMessage(content = \"Hi , My name is Ashis and I am a upcoming software Engineer \") ,\n",
    "    AIMessage(content = \"Hi Ashis\") ,\n",
    "    HumanMessage(content = \"I like chocolate icecreme \") ,\n",
    "    AIMessage(content=\"nice\") ,\n",
    "    HumanMessage(content=\"I like to play cricket\") ,\n",
    "    AIMessage(content=\"That's awesome\") ,\n",
    "    HumanMessage(content=\"I am from India\") ,\n",
    "    AIMessage(content=\"That's cool\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like to play cricket', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's awesome\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am from India', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's cool\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you love to play **cricket**! üèè  \\n\\nIs there anything else you enjoy doing besides cricket?\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if using chain\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign( messages=itemgetter(\"messages\") | trimmer ) | prompt | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What I love to play ?\")],\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you are from India!  üáÆüá≥ üèè \\n\\n\\nLet me know if you have any other questions about cricket or anything else. üòä\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets wrap this message history \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\" : {\"session_id\": \"chat5\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What country I belongs to ?\")],\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    "    ,config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with VectorStrores and Retriever\n",
    "\n",
    "Langchain implement a Document Abstraction which is intended to represent a unit of text and associated metadata . It has two attributes :-\n",
    "\n",
    "- page_content: a string representing the content \n",
    "- metadata: a dict containing arbitrary metadata ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Document represent a chunk of large page , each Document can reprsent a page of a book .\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain simplifies working with LLMs.\",\n",
    "        metadata={\"source\": \"blog\", \"author\": \"John Doe\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"AI is transforming industries worldwide.\",\n",
    "        metadata={\"source\": \"news\", \"date\": \"2025-03-20\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"OpenAI's ChatGPT is a powerful language model.\",\n",
    "        metadata={\"source\": \"research\", \"keywords\": \"OpenAI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning models require large datasets for training.\",\n",
    "        metadata={\"source\": \"research_paper\", \"publisher\": \"IEEE\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep learning has revolutionized image and speech recognition.\",\n",
    "        metadata={\"source\": \"conference\", \"event\": \"NeurIPS 2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Quantum computing could potentially accelerate AI computations.\",\n",
    "        metadata={\"source\": \"tech_magazine\", \"issue\": \"March 2025\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cybersecurity is becoming increasingly important in the AI era.\",\n",
    "        metadata={\"source\": \"whitepaper\", \"organization\": \"CyberSec Corp\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'blog', 'author': 'John Doe'}, page_content='LangChain simplifies working with LLMs.'),\n",
       " Document(metadata={'source': 'news', 'date': '2025-03-20'}, page_content='AI is transforming industries worldwide.'),\n",
       " Document(metadata={'source': 'research', 'keywords': 'OpenAI'}, page_content=\"OpenAI's ChatGPT is a powerful language model.\"),\n",
       " Document(metadata={'source': 'research_paper', 'publisher': 'IEEE'}, page_content='Machine learning models require large datasets for training.'),\n",
       " Document(metadata={'source': 'conference', 'event': 'NeurIPS 2024'}, page_content='Deep learning has revolutionized image and speech recognition.'),\n",
       " Document(metadata={'source': 'tech_magazine', 'issue': 'March 2025'}, page_content='Quantum computing could potentially accelerate AI computations.'),\n",
       " Document(metadata={'source': 'whitepaper', 'organization': 'CyberSec Corp'}, page_content='Cybersecurity is becoming increasingly important in the AI era.')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using huggingFace , we can call open source embeddings and LLMs\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020F90D738E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020F90D72080>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"Gemma2-9b-It\" , groq_api_key=groq_api_key)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.29.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.3.45)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (3.4.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.21.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (4.49.0)\n",
      "Requirement already satisfied: filelock in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.3.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2025.1.31)\n",
      "Requirement already satisfied: networkx in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\91993\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\complete generative ai\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Before storing converting into embeddings \n",
    "# wrt hugging face \n",
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"all-MiniLM-L6-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorStores \n",
    "# Here everything will get converted into vectors and embeddings and then store it into vectorStores \n",
    "\n",
    "from langchain_chroma import Chroma \n",
    "vectorstore = Chroma.from_documents(documents , embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b1adf05b-e8cd-48ef-926e-2701c1640871', metadata={'keywords': 'OpenAI', 'source': 'research'}, page_content=\"OpenAI's ChatGPT is a powerful language model.\"),\n",
       " Document(id='4714fc34-66a5-4da8-90aa-e6c29c46597f', metadata={'author': 'John Doe', 'source': 'blog'}, page_content='LangChain simplifies working with LLMs.'),\n",
       " Document(id='0f1f65aa-8bd4-411a-8f87-ea2a04e3fca8', metadata={'date': '2025-03-20', 'source': 'news'}, page_content='AI is transforming industries worldwide.'),\n",
       " Document(id='8ed66233-4e57-4776-986e-b36d67ed6b05', metadata={'event': 'NeurIPS 2024', 'source': 'conference'}, page_content='Deep learning has revolutionized image and speech recognition.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"OpenAI's ChatGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b1adf05b-e8cd-48ef-926e-2701c1640871', metadata={'keywords': 'OpenAI', 'source': 'research'}, page_content=\"OpenAI's ChatGPT is a powerful language model.\"),\n",
       " Document(id='4714fc34-66a5-4da8-90aa-e6c29c46597f', metadata={'author': 'John Doe', 'source': 'blog'}, page_content='LangChain simplifies working with LLMs.'),\n",
       " Document(id='0f1f65aa-8bd4-411a-8f87-ea2a04e3fca8', metadata={'date': '2025-03-20', 'source': 'news'}, page_content='AI is transforming industries worldwide.'),\n",
       " Document(id='8ed66233-4e57-4776-986e-b36d67ed6b05', metadata={'event': 'NeurIPS 2024', 'source': 'conference'}, page_content='Deep learning has revolutionized image and speech recognition.')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for async query \n",
    "await vectorstore.asimilarity_search(\"OpenAI's ChatGPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers\n",
    "\n",
    "- Langchain VectorStore objects do not subclass Runnable , so cannot immediately be inegrated into Langchain Expression Language chains .\n",
    "- But Langchain Retrievers are subclass Runnables so they implement a standard set of measurement (e.g synchronous and asynchronous  invoke and batch operation ) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='b1adf05b-e8cd-48ef-926e-2701c1640871', metadata={'keywords': 'OpenAI', 'source': 'research'}, page_content=\"OpenAI's ChatGPT is a powerful language model.\")],\n",
       " [Document(id='6b16486b-f0d0-47f9-b645-bbf47324cd26', metadata={'publisher': 'IEEE', 'source': 'research_paper'}, page_content='Machine learning models require large datasets for training.')]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)     # It will iterate over the batch and get the first matching element .\n",
    "retriever.batch(['OpenAi' , 'Machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8ed66233-4e57-4776-986e-b36d67ed6b05', metadata={'event': 'NeurIPS 2024', 'source': 'conference'}, page_content='Deep learning has revolutionized image and speech recognition.')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now how to quesry from the vectorstore , best way is to take from retriever \n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\" , \n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "\n",
    "retriever.invoke(\"Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaining the retriever \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"    \n",
    "\n",
    "Answer this question using the provided context only .\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\" , message)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\" : retriever , \"question\":RunnablePassthrough() } | prompt | llm   \n",
    "# Here the retriever has all the info from the vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"tell me about OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI created ChatGPT, a powerful language model.  \\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
